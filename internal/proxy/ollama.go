package proxy

import (
	"fmt"
	"log/slog"
	"net/http"
	"strings"
	"time"

	"github.com/n0madic/go-chatmock/internal/config"
	"github.com/n0madic/go-chatmock/internal/models"
	"github.com/n0madic/go-chatmock/internal/sse"
	"github.com/n0madic/go-chatmock/internal/transform"
	"github.com/n0madic/go-chatmock/internal/types"
	"github.com/n0madic/go-chatmock/internal/upstream"
)

func (s *Server) handleOllamaVersion(w http.ResponseWriter, r *http.Request) {
	writeJSON(w, http.StatusOK, types.OllamaVersionResponse{Version: config.OllamaVersionString})
}

func (s *Server) handleOllamaTags(w http.ResponseWriter, r *http.Request) {
	mods := s.Registry.GetModels()
	var modelList []types.OllamaModelEntry
	for _, m := range mods {
		if m.Visibility == "hidden" {
			continue
		}
		ids := []string{m.Slug}
		if s.Config.ExposeReasoningModels {
			for _, lvl := range m.SupportedReasoningLevels {
				ids = append(ids, m.Slug+"-"+lvl.Effort)
			}
		}
		for _, id := range ids {
			modelList = append(modelList, types.OllamaModelEntry{
				Name:       id,
				Model:      id,
				ModifiedAt: "2023-10-01T00:00:00Z",
				Size:       815319791,
				Digest:     "8648f39daa8fbf5b18c7b4e6a8fb4990c692751d49917417b8842ca5758e7ffc",
				Details: types.OllamaModelDetails{
					ParentModel:       "",
					Format:            "gguf",
					Family:            "llama",
					Families:          []string{"llama"},
					ParameterSize:     "8.0B",
					QuantizationLevel: "Q4_0",
				},
			})
		}
	}
	writeJSON(w, http.StatusOK, types.OllamaModelList{Models: modelList})
}

func (s *Server) handleOllamaShow(w http.ResponseWriter, r *http.Request) {
	var payload map[string]any
	if _, ok := parseJSONRequest(w, r, &payload, writeError, "Failed to read request body", "Invalid JSON body"); !ok {
		return
	}
	model, _ := payload["model"].(string)
	if model == "" {
		writeOllamaError(w, http.StatusBadRequest, "Model not found")
		return
	}

	writeJSON(w, http.StatusOK, types.OllamaShowResponse{
		Modelfile:  "# Modelfile generated by \"ollama show\"\nFROM /models/blobs/sha256:placeholder\nTEMPLATE \"\"\"{{ .System }}\\nUSER: {{ .Prompt }}\\nASSISTANT: \"\"\"\nPARAMETER num_ctx 100000\nPARAMETER stop \"</s>\"\nPARAMETER stop \"USER:\"\nPARAMETER stop \"ASSISTANT:\"",
		Parameters: "num_keep 24\nstop \"<|start_header_id|>\"\nstop \"<|end_header_id|>\"\nstop \"<|eot_id|>\"",
		Template:   "{{ if .System }}<|start_header_id|>system<|end_header_id|>\n\n{{ .System }}<|eot_id|>{{ end }}{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>\n\n{{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>\n\n{{ .Response }}<|eot_id|>",
		Details: types.OllamaModelDetails{
			ParentModel:       "",
			Format:            "gguf",
			Family:            "llama",
			Families:          []string{"llama"},
			ParameterSize:     "8.0B",
			QuantizationLevel: "Q4_0",
		},
		ModelInfo: map[string]any{
			"general.architecture": "llama",
			"general.file_type":    2,
			"llama.context_length": 2000000,
		},
		Capabilities: []string{"completion", "vision", "tools", "thinking"},
	})
}

func (s *Server) handleOllamaChat(w http.ResponseWriter, r *http.Request) {
	var payload map[string]any
	if _, ok := parseJSONRequest(w, r, &payload, writeOllamaError, "Failed to read request body", "Invalid JSON body"); !ok {
		return
	}

	modelName, _ := payload["model"].(string)

	// Convert Ollama messages to OpenAI format
	rawMsgs, _ := payload["messages"].([]any)
	var topImages []string
	if imgs, ok := payload["images"].([]any); ok {
		for _, img := range imgs {
			if s, ok := img.(string); ok {
				topImages = append(topImages, s)
			}
		}
	}
	messages := transform.ConvertOllamaMessages(rawMsgs, topImages)
	convertSystemToUser(messages)

	// Stream defaults to true for Ollama
	streamReq := true
	if v, ok := payload["stream"].(bool); ok {
		streamReq = v
	}

	// Tools
	toolsRaw, _ := payload["tools"].([]any)
	normalizedTools := transform.NormalizeOllamaTools(toolsRaw)
	toolsResponses := transform.ToolsChatToResponses(normalizedTools)
	toolChoice := "auto"
	if tc, ok := payload["tool_choice"].(string); ok {
		toolChoice = tc
	}
	parallelToolCalls := boolVal(payload, "parallel_tool_calls")

	// Passthrough responses_tools
	rtPayload, _ := payload["responses_tools"].([]any)
	rtChoice, _ := payload["responses_tool_choice"].(string)
	extraTools, hadResponsesTools := s.extractOllamaResponsesTools(rtPayload, rtChoice)
	if extraTools == nil && hadResponsesTools {
		writeOllamaError(w, http.StatusBadRequest, "Only web_search/web_search_preview are supported in responses_tools")
		return
	}
	defaultWebSearchApplied := rtPayload == nil && len(extraTools) > 0 && s.Config.DefaultWebSearch && rtChoice != "none"
	if len(extraTools) > 0 {
		toolsResponses = append(toolsResponses, extraTools...)
	}

	if rtc := rtChoice; rtc == "auto" || rtc == "none" {
		toolChoice = rtc
	}

	if modelName == "" || len(messages) == 0 {
		writeOllamaError(w, http.StatusBadRequest, "Invalid request format")
		return
	}

	// Convert to Responses input
	inputItems := transform.ChatMessagesToResponsesInput(messages)

	normalizedModel := models.NormalizeModelName(modelName, s.Config.DebugModel)

	if ok, hint := s.Registry.IsKnownModel(normalizedModel); !ok {
		msg := fmt.Sprintf("model %q is not available via this endpoint", normalizedModel)
		if hint != "" {
			msg += "; available models: " + hint
		}
		writeOllamaError(w, http.StatusBadRequest, msg)
		return
	}

	reasoningParam := buildReasoningWithModelFallback(s.Config, modelName, modelName, nil)
	reasoningEffort, reasoningSummary := reasoningLogFields(reasoningParam)
	if s.Config.Verbose {
		slog.Info("ollama.chat.request",
			"requested_model", modelName,
			"upstream_model", normalizedModel,
			"stream", streamReq,
			"messages", len(rawMsgs),
			"input_items", len(inputItems),
			"images", len(topImages),
			"tools", len(toolsResponses),
			"tool_choice", summarizeToolChoice(toolChoice),
			"responses_tools", len(extraTools),
			"default_web_search", defaultWebSearchApplied,
			"parallel_tool_calls", parallelToolCalls,
			"reasoning_effort", reasoningEffort,
			"reasoning_summary", reasoningSummary,
			"session_override", strings.TrimSpace(r.Header.Get("X-Session-Id")) != "",
		)
	}

	upReq := &upstream.Request{
		Model:             normalizedModel,
		Instructions:      s.Config.InstructionsForModel(normalizedModel),
		InputItems:        inputItems,
		Tools:             toolsResponses,
		ToolChoice:        toolChoice,
		ParallelToolCalls: parallelToolCalls,
		ReasoningParam:    reasoningParam,
		SessionID:         r.Header.Get("X-Session-Id"),
	}

	baseTools := transform.ToolsChatToResponses(normalizedTools)
	resp, ok := s.doUpstreamWithResponsesToolsRetry(r.Context(), w, upReq, hadResponsesTools, baseTools, writeOllamaError)
	if !ok {
		return
	}

	createdAt := time.Now().UTC().Format("2006-01-02T15:04:05Z")
	modelOut := modelName

	if streamReq {
		w.Header().Set("Content-Type", "application/x-ndjson")
		w.WriteHeader(http.StatusOK)
		sse.TranslateOllama(w, resp.Body.Body, modelOut, createdAt, sse.TranslateOllamaOptions{
			ReasoningCompat: s.Config.ReasoningCompat,
		})
		return
	}

	// Non-streaming Ollama
	s.collectOllamaChat(w, resp, normalizedModel, createdAt)
}

func (s *Server) collectOllamaChat(w http.ResponseWriter, resp *upstream.Response, normalizedModel, createdAt string) {
	collected := collectTextResponseFromSSE(resp.Body.Body, collectTextResponseOptions{
		CollectReasoning: true,
		CollectToolCalls: true,
	})
	fullText := collected.FullText
	compat := s.Config.ReasoningCompat
	if compat == "" || compat == "think-tags" {
		var parts []string
		if collected.ReasoningSummary != "" {
			parts = append(parts, collected.ReasoningSummary)
		}
		if collected.ReasoningFull != "" {
			parts = append(parts, collected.ReasoningFull)
		}
		if len(parts) > 0 {
			var rtxt strings.Builder
			for i, p := range parts {
				if i > 0 {
					rtxt.WriteString("\n\n")
				}
				rtxt.WriteString(p)
			}
			fullText = "<think>" + rtxt.String() + "</think>" + fullText
		}
	}

	chunk := types.OllamaStreamChunk{
		Model:          normalizedModel,
		CreatedAt:      createdAt,
		Message:        types.OllamaMessage{Role: "assistant", Content: fullText, ToolCalls: collected.ToolCalls},
		Done:           true,
		DoneReason:     "stop",
		OllamaFakeEval: types.OllamaFakeEvalDefaults,
	}

	writeJSON(w, http.StatusOK, chunk)
}

func (s *Server) extractOllamaResponsesTools(rtPayload []any, rtChoice string) ([]types.ResponsesTool, bool) {
	return s.extractResponsesTools(rtPayload, rtChoice)
}
