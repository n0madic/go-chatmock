package proxy

import (
	"encoding/json"
	"fmt"
	"io"
	"log/slog"
	"net/http"
	"strings"
	"time"

	"github.com/n0madic/go-chatmock/internal/config"
	"github.com/n0madic/go-chatmock/internal/limits"
	"github.com/n0madic/go-chatmock/internal/models"
	"github.com/n0madic/go-chatmock/internal/reasoning"
	"github.com/n0madic/go-chatmock/internal/sse"
	"github.com/n0madic/go-chatmock/internal/transform"
	"github.com/n0madic/go-chatmock/internal/types"
	"github.com/n0madic/go-chatmock/internal/upstream"
)

func (s *Server) handleOllamaVersion(w http.ResponseWriter, r *http.Request) {
	writeJSON(w, http.StatusOK, types.OllamaVersionResponse{Version: config.OllamaVersionString})
}

func (s *Server) handleOllamaTags(w http.ResponseWriter, r *http.Request) {
	mods := s.Registry.GetModels()
	var modelList []types.OllamaModelEntry
	for _, m := range mods {
		if m.Visibility == "hidden" {
			continue
		}
		ids := []string{m.Slug}
		if s.Config.ExposeReasoningModels {
			for _, lvl := range m.SupportedReasoningLevels {
				ids = append(ids, m.Slug+"-"+lvl.Effort)
			}
		}
		for _, id := range ids {
			modelList = append(modelList, types.OllamaModelEntry{
				Name:       id,
				Model:      id,
				ModifiedAt: "2023-10-01T00:00:00Z",
				Size:       815319791,
				Digest:     "8648f39daa8fbf5b18c7b4e6a8fb4990c692751d49917417b8842ca5758e7ffc",
				Details: types.OllamaModelDetails{
					ParentModel:       "",
					Format:            "gguf",
					Family:            "llama",
					Families:          []string{"llama"},
					ParameterSize:     "8.0B",
					QuantizationLevel: "Q4_0",
				},
			})
		}
	}
	writeJSON(w, http.StatusOK, types.OllamaModelList{Models: modelList})
}

func (s *Server) handleOllamaShow(w http.ResponseWriter, r *http.Request) {
	body, err := io.ReadAll(r.Body)
	if err != nil {
		writeError(w, http.StatusBadRequest, "Failed to read request body")
		return
	}
	var payload map[string]any
	if err := json.Unmarshal(body, &payload); err != nil {
		writeError(w, http.StatusBadRequest, "Invalid JSON body")
		return
	}
	model, _ := payload["model"].(string)
	if model == "" {
		writeOllamaError(w, http.StatusBadRequest, "Model not found")
		return
	}

	writeJSON(w, http.StatusOK, types.OllamaShowResponse{
		Modelfile:  "# Modelfile generated by \"ollama show\"\nFROM /models/blobs/sha256:placeholder\nTEMPLATE \"\"\"{{ .System }}\\nUSER: {{ .Prompt }}\\nASSISTANT: \"\"\"\nPARAMETER num_ctx 100000\nPARAMETER stop \"</s>\"\nPARAMETER stop \"USER:\"\nPARAMETER stop \"ASSISTANT:\"",
		Parameters: "num_keep 24\nstop \"<|start_header_id|>\"\nstop \"<|end_header_id|>\"\nstop \"<|eot_id|>\"",
		Template:   "{{ if .System }}<|start_header_id|>system<|end_header_id|>\n\n{{ .System }}<|eot_id|>{{ end }}{{ if .Prompt }}<|start_header_id|>user<|end_header_id|>\n\n{{ .Prompt }}<|eot_id|>{{ end }}<|start_header_id|>assistant<|end_header_id|>\n\n{{ .Response }}<|eot_id|>",
		Details: types.OllamaModelDetails{
			ParentModel:       "",
			Format:            "gguf",
			Family:            "llama",
			Families:          []string{"llama"},
			ParameterSize:     "8.0B",
			QuantizationLevel: "Q4_0",
		},
		ModelInfo: map[string]any{
			"general.architecture": "llama",
			"general.file_type":    2,
			"llama.context_length": 2000000,
		},
		Capabilities: []string{"completion", "vision", "tools", "thinking"},
	})
}

func (s *Server) handleOllamaChat(w http.ResponseWriter, r *http.Request) {
	body, err := io.ReadAll(http.MaxBytesReader(w, r.Body, maxBodyBytes))
	if err != nil {
		writeError(w, http.StatusBadRequest, "Failed to read request body")
		return
	}

	if s.Config.Verbose {
		slog.Info("IN POST /api/chat", "body", string(body))
	}

	var payload map[string]any
	if err := json.Unmarshal(body, &payload); err != nil {
		writeOllamaError(w, http.StatusBadRequest, "Invalid JSON body")
		return
	}

	modelName, _ := payload["model"].(string)

	// Convert Ollama messages to OpenAI format
	rawMsgs, _ := payload["messages"].([]any)
	var topImages []string
	if imgs, ok := payload["images"].([]any); ok {
		for _, img := range imgs {
			if s, ok := img.(string); ok {
				topImages = append(topImages, s)
			}
		}
	}
	messages := transform.ConvertOllamaMessages(rawMsgs, topImages)
	convertSystemToUser(messages)

	// Stream defaults to true for Ollama
	streamReq := true
	if v, ok := payload["stream"].(bool); ok {
		streamReq = v
	}

	// Tools
	toolsRaw, _ := payload["tools"].([]any)
	normalizedTools := transform.NormalizeOllamaTools(toolsRaw)
	toolsResponses := transform.ToolsChatToResponses(normalizedTools)
	toolChoice := "auto"
	if tc, ok := payload["tool_choice"].(string); ok {
		toolChoice = tc
	}
	parallelToolCalls := boolVal(payload, "parallel_tool_calls")

	// Passthrough responses_tools
	rtPayload, _ := payload["responses_tools"].([]any)
	rtChoice, _ := payload["responses_tool_choice"].(string)
	extraTools, hadResponsesTools := s.extractOllamaResponsesTools(rtPayload, rtChoice)
	if extraTools == nil && hadResponsesTools {
		writeOllamaError(w, http.StatusBadRequest, "Only web_search/web_search_preview are supported in responses_tools")
		return
	}
	if len(extraTools) > 0 {
		toolsResponses = append(toolsResponses, extraTools...)
	}

	if rtc := rtChoice; rtc == "auto" || rtc == "none" {
		toolChoice = rtc
	}

	if modelName == "" || len(messages) == 0 {
		writeOllamaError(w, http.StatusBadRequest, "Invalid request format")
		return
	}

	// Convert to Responses input
	inputItems := transform.ChatMessagesToResponsesInput(messages)

	modelReasoning := reasoning.ExtractFromModelName(modelName)
	normalizedModel := models.NormalizeModelName(modelName, s.Config.DebugModel)

	if ok, hint := s.Registry.IsKnownModel(normalizedModel); !ok {
		msg := fmt.Sprintf("model %q is not available via this endpoint", normalizedModel)
		if hint != "" {
			msg += "; available models: " + hint
		}
		writeOllamaError(w, http.StatusBadRequest, msg)
		return
	}

	reasoningParam := reasoning.BuildReasoningParam(
		s.Config.ReasoningEffort,
		s.Config.ReasoningSummary,
		modelReasoning,
		modelName,
	)

	upReq := &upstream.Request{
		Model:             normalizedModel,
		Instructions:      s.Config.InstructionsForModel(normalizedModel),
		InputItems:        inputItems,
		Tools:             toolsResponses,
		ToolChoice:        toolChoice,
		ParallelToolCalls: parallelToolCalls,
		ReasoningParam:    reasoningParam,
		SessionID:         r.Header.Get("X-Session-Id"),
	}

	resp, err := s.upstreamClient.Do(r.Context(), upReq)
	if err != nil {
		writeOllamaError(w, http.StatusUnauthorized, err.Error())
		return
	}
	limits.RecordFromResponse(resp.Headers)

	if resp.StatusCode >= 400 {
		baseTools := transform.ToolsChatToResponses(normalizedTools)
		var ok bool
		resp, ok = s.doWithRetry(r.Context(), w, resp, upReq, hadResponsesTools, baseTools, writeOllamaError)
		if !ok {
			return
		}
	}

	createdAt := time.Now().UTC().Format("2006-01-02T15:04:05Z")
	modelOut := modelName

	if streamReq {
		w.Header().Set("Content-Type", "application/x-ndjson")
		w.WriteHeader(http.StatusOK)
		sse.TranslateOllama(w, resp.Body.Body, modelOut, createdAt, sse.TranslateOllamaOptions{
			ReasoningCompat: s.Config.ReasoningCompat,
			Verbose:         s.Config.Verbose,
		})
		return
	}

	// Non-streaming Ollama
	s.collectOllamaChat(w, resp, modelOut, normalizedModel, createdAt)
}

func (s *Server) collectOllamaChat(w http.ResponseWriter, resp *upstream.Response, modelOut, normalizedModel, createdAt string) {
	defer resp.Body.Body.Close()

	reader := sse.NewReader(resp.Body.Body)
	var fullText string
	var reasoningSummaryText string
	var reasoningFullText string
	var toolCalls []types.ToolCall

	for {
		evt, err := reader.Next()
		if err != nil {
			break
		}
		switch evt.Type {
		case "response.output_text.delta":
			delta, _ := evt.Data["delta"].(string)
			fullText += delta
		case "response.reasoning_summary_text.delta":
			delta, _ := evt.Data["delta"].(string)
			reasoningSummaryText += delta
		case "response.reasoning_text.delta":
			delta, _ := evt.Data["delta"].(string)
			reasoningFullText += delta
		case "response.output_item.done":
			item, _ := evt.Data["item"].(map[string]any)
			if t, _ := item["type"].(string); t == "function_call" {
				callID := stringOrEmpty(item, "call_id")
				if callID == "" {
					callID = stringOrEmpty(item, "id")
				}
				name := stringOrEmpty(item, "name")
				args := stringOrEmpty(item, "arguments")
				if callID != "" && name != "" {
					toolCalls = append(toolCalls, types.ToolCall{
						ID: callID, Type: "function",
						Function: types.FunctionCall{Name: name, Arguments: args},
					})
				}
			}
		case "response.completed":
			goto ollamaDone
		}
	}

ollamaDone:
	compat := s.Config.ReasoningCompat
	if compat == "" || compat == "think-tags" {
		var parts []string
		if reasoningSummaryText != "" {
			parts = append(parts, reasoningSummaryText)
		}
		if reasoningFullText != "" {
			parts = append(parts, reasoningFullText)
		}
		if len(parts) > 0 {
			var rtxt strings.Builder
			for i, p := range parts {
				if i > 0 {
					rtxt.WriteString("\n\n")
				}
				rtxt.WriteString(p)
			}
			fullText = "<think>" + rtxt.String() + "</think>" + fullText
		}
	}

	chunk := types.OllamaStreamChunk{
		Model:          normalizedModel,
		CreatedAt:      createdAt,
		Message:        types.OllamaMessage{Role: "assistant", Content: fullText, ToolCalls: toolCalls},
		Done:           true,
		DoneReason:     "stop",
		OllamaFakeEval: types.OllamaFakeEvalDefaults,
	}

	writeJSON(w, http.StatusOK, chunk)
}

func (s *Server) extractOllamaResponsesTools(rtPayload []any, rtChoice string) ([]types.ResponsesTool, bool) {
	return s.extractResponsesTools(rtPayload, rtChoice)
}
